\documentclass[../main.tex]{subfiles}

\begin{document}

In this chapter we confront the goals that were set at the beginning of the thesis with the actual result.
We outline advantages and disadvantages of the final solution while discussing possible future developments to improve the performance of the current design.

\newpage
\section{Design flow}
At the beginning of the thesis we discussed the main advantages of deploying Deep Learning models on FPGA.
We discussed how the characteristics of FPGAs make them ideal targets to deploy sparse model with custom precision while being energy efficient.
The drawbacks of FPGAs as HW solution were presented as two-fold: the first problem is related to the long design required to produce a RTL design and deploy the model; this also do not allow fast prototyping of different solution which is a desirable characteristic of the development process.
The second problem is related to the performance of the final result; just deploying the model as a sequence of transformations on tensors is highly inefficient. The target would be required to store buffers with the size of an entire tensor and all the operations would require loading off-chip memory leading to very poor data locality.
Since memory transfer is usually responsible for most of the power consumption continuous communication with the off-chip memory would defeat the purpose of using FPGAs in application when the power consumption is a requirement of the final application \cite{DBLP:journals/corr/abs-1901-04988}.

The solution to the first problem is the proposed design flow itself.
The proposed stack ease the design process by only requiring the programmer to convert the computational graph of the Deep Learning model to its ONNX IR representation.
The model can then be optimized by performing weight pruning and quantization; the Halide infrastructure optimize and translate the ONNX model to a software representation and the PandA-Bambu framework translate the software representation to Verilog/VHDL code.

The second problem is addressed by the scheduling of the Halide infrastructure.
One of the main characteristics of halide is the separation of the algorithm definition and the scheduling. 
The scheduling allow to optimize the computation by finding the right trade-off among data locality, parallelism and buffer sizes.
Halide also allow to automate the process by using an autoscheduler \cite{halideAutoscheduler},

\section{Future developments and current limitations}

The main current limitation is the lack of an autoscheduler for FPGA targets. 
Other autoschedulers are already available (Current only for x86 CPUs) but they do not take into consideration the HW resource of the target FPGA and try to predict characteristics that make sense only in the CPU case (Such as number of page fault costs) \cite{halideAutoscheduler}.
An other disadvantage of currently available autoschedulers is the fact that they do not take into consideration the HW limitations of the target; for complex models the autoschedule easily produce schedule with buffer sizes that exceed the available memory.
Using an autoscheduler that take into consideration the HW resources and limitations would result in a significant improvement in terms of ease of use and final quality of results.

An other limitation is the simplicity of how the back-end extract queue channels between filters.
The current implementation use queues only in the most simple case; single read/write access, parallelization under same variables, same loop nest and matching read/write indexes.
A more complete implementation should be able to extract queues between filters that do not have such static and strict requirements.

Pruning and quantization operations are currently performed by relying on the VitisAI tools and thus only limited to Xilinx Hardware.
Developing other tools able to optimize DL models for generic targets and not only for Xilinx HW would allow the programmer to optimize models without using other resources.

In terms of support for Deep Learning models, the current implementation support models that implemented by the ONNX to Halide converted implemented inside the Halide infrastructure and some types of layers are not supported.
That limitation affect the type of models that can be deployed by using the proposed design flow, but Halide is currently under development and will improve the support in the future.

\end{document}