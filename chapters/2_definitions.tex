\documentclass[../main.tex]{subfiles}

\begin{document}
This chapter introduces some preliminary concepts. 
Section \ref{AI-ML} introduces the early ideas in Artificial Intelligence and how they started advancements that led to the state of the art methods.
Section \ref{ANN} introduces the Artificial Neural Network (ANN) model and how it is used in the Machine Learning context; we introduce the concept of Deep Learning and how Convolutional Neural Networks can be used to perform pattern recognition on images and spatially organized features. 
We also introduce the problem of deploying Deep Learning models in resource-constrained applications.
Sections \ref{ONNX} introduce the ONNX IR and why we need an intermediate representation for Deep Learning frameworks.

\newpage

\section{Artificial Intelligence and Machine Learning}
\label{AI-ML}

The Artificial Intelligence field is born in the 1950s with the goal of creating thought-capable artificial beings. 
With the creation of the computational model of the Turing machine, the Computer Science field started studying how machines can act humanly and solve problems that were previously considered as an exclusive domain of human intelligence.
Early efforts to create agents capable of showing intelligent behavior were directed toward creating decision-making algorithms, mostly focused on solving games with graph representations.
Even if the first results were able to solve problems that were previously considered unsolvable, the field soon realized that the approach was not well suited to solve different and more general problems.

Even if the early approaches were too general and failed to realize how complex the problem of creating an Artificial General Intelligence is they sparked the emerging field of Machine Learning. 
The Computer Science field realized that the graph-based approach lacks the flexibility that is needed to create agents that show intelligent behavior; humans do not interact with their environment by optimizing a utility function and performing search procedures in a graph of possible states. 
Intelligent agents should be able to learn directly from the environment how to perform the computation needed to solve a specific problem.


The Machine Learning field changed the AI perspective by changing the paradigm of how the computation is performed. 
The program is not seen as given input of the system and the process is split into training and inference phases. 
The training phase uses a set of sample data to learn how to perform a task without being explicitly programmed to solve it. 
The learned model must then be able to solve the same problem while being able to generalize on previously unseen samples.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[]{0.4\textwidth}
        \includegraphics[width=\textwidth]{images/ML1.png}
        \caption{Traditional programming}
    \end{subfigure}
    \hspace{.1\linewidth}
    \begin{subfigure}[]{0.4\textwidth}
        \includegraphics[width=\textwidth]{images/ML2.png}
        \caption{Machine Learning}
        \label{fig:ML2}
    \end{subfigure}
    \caption{ Comparison between the Traditional Programming approach and the Machine Learning approach}
    \label{fig:ML}
\end{figure}

Figure (a) show how the computation is usually performed in a traditional program execution; the computer produces the output by taking the program and the data as input. As a comparison figure (b) shows how the Machine Learning approach is fundamentally different. The program is not an input of the computer; the program is the output of the learning phase and is meant to be used to generalize on unseen data.

By splitting the process into training and inference phases the algorithm learn a representation of the task to be used later as previous knowledge of the problem. 
Moreover, since the ML approach does not specify the kind of model that needs to be used it can adapt by using the model that better fits a given task.

One of the most interesting models generated by the ML field is the Artificial Neural Network that is going to be presented in the next section.

\section{Artificial Neural Networks}
\label{ANN}

Artificial Neural Networks are powerful Machine Learning models inspired by how animal brains work.
ANNs are data driven models composed of artificial neurons and synapses that perform nonlinear transformations of input data.
This section presents how ANNs work and how they evolved to solve more complex problems by using DL and CNN layers.

\subsection{Vanilla neural networks}
An Artificial Neural network is composed of multiple interconnected neurons. Each neuron is based on the perceptron model; a model invented in the 1950s to perform binary classification on input features.

\bigskip
\bigskip
\begin{figure}[h!]
  \centering
  %\subfigure{\includegraphics[width=.4\linewidth]{images/Perceptron.png}}
  %\subfigure{ $f(x) = x^2$ \vspace*{-20cm}}
  \begin{subfigure}[]{0.4\textwidth}
    \includegraphics[width=\linewidth]{images/Perceptron.png}
  \end{subfigure}
  \begin{subfigure}[]{0.4\textwidth}
    \includegraphics[width=\linewidth]{images/PerceptronFormula.png}
  \end{subfigure}
  \caption{Perceptron model}
  \label{fig:Perceptron}
\end{figure}

\newpage
To learn nonlinear representations of the input data the perceptron model uses a nonlinear activation function before propagating the output value to the output connections. Two simple and common activation functions are the sigmoid and tanh functions.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[]{0.4\textwidth}
        \includegraphics[width=\linewidth]{images/SigmoidFunction.png}
        \caption{$ {y = \frac{1}{(1+e^{-x})}} $}
        \label{fig:Sigmoid}
    \end{subfigure}
    \hspace{1cm}
    \begin{subfigure}[]{0.4\textwidth}
        \includegraphics[width=\linewidth]{images/TanhFunction.png}
        \caption{$ {y = \tanh(x)} $}
        \label{fig:Tanh}
    \end{subfigure}
    \caption{Sigmoid (a) and Tanh (b) functions}
    \label{fig:SigmoidTanh}
\end{figure}


The perceptron model can be composed creating a network of interconnected neurons, usually referred to as Multi-Layer Perceptron.

\begin{figure}[h!]
  \centering
  \includegraphics[width=.6\linewidth]{images/neural_net2.jpeg}
  \caption{Example of Multi-layer Perceptron network. \cite{CS231n}}
  \label{fig:MLP}
\end{figure}

The MLP model uses a topology named Feed Forward configuration. 
A FF neural network is an MLP composed of a sequence of fully connected layers; a FF network always has one input and one output layer with a set of hidden layers in between.

Since the ANN model is computationally demanding and its use has been held back by the limited computational resources available, the use of the Feed Forward configuration is usually preferred to exploit SIMD instructions.
By seeing the network as a sequence of transformations over the input features, the algorithm can easily parallelize the training and inference phases on specialized HW by using SIMD instructions.
This allows the use of bigger and more complex networks leading to better performing models.


\subsection{ANN Training}
As previously stated a Machine Learning model need to be trained on a set of training data before being used with new and previously unseen samples.
The most common method to train a Neural Network is through supervised learning using the back-propagation algorithm.

The back-propagation algorithm is a gradient-based optimization method.
Since the Neural Network is a differentiable model, given a differentiable function the back-propagation algorithm can calculate the network gradient (Also known as backward-pass) and iteratively optimize the network weights toward values corresponding to lower loss values.

The back-propagation algorithm is able to learn a set of weights that approximate the desired output, even in the case of Neural Networks with multiple hidden layers.
The weights belonging to hidden layers can not be directly calculated; the back-propagation algorithm needs to use the chain rule to calculate the gradient of hidden layers.
This leads to the problem of the vanishing gradient.
Since the gradient vanishes while propagating through the layers this limit how deep a Neural Network can be. 
The gradient shrinks and layers distant to the output get trained more slowly than layers close to the output.

The problem is solved by using activation functions that do not shrink the gradient at each layer propagation; a possible example is the use of the ReLU activation function.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[c]{0.4\textwidth}
        \includegraphics[width=\linewidth]{images/ReLU.png}
        \caption{$ {y = \max{(0,}{ x)}} $}
        \label{fig:ReLy}
    \end{subfigure}
    \hspace{1cm}
    \begin{subfigure}[c]{0.4\textwidth}
        \includegraphics[width=\linewidth]{images/LeakyReLU.png}
        \caption{$ {y = } \left\{
            \begin{array}{ll}
                x & x\geq 0 \\
                ax & otherwise 
            \end{array} \right. $}
        \label{fig:LeakyReLU}
    \end{subfigure}
    \caption{Relu (a) and Leaky ReLU (b) functions.  }
    \label{fig:ReLUs}
\end{figure}

The ReLU function is introduced to avoid the problem of the vanishing gradient while being able to introduce nonlinearities. Since the ReLU function has gradient equal to $0$ when $x\le 0$ most of the connections become deactivated during the training phase. To solve this problem multiple activation functions have been proposed; one example is the Leaky ReLU function that assigns a small gradient $a\leq 1$ when $x$ is negative.

\subsection{Deep learning and Convolutional Neural Networks}

The Artificial Neural Networks are powerful models to process high dimensional features but there are situations where the MLP model would require too many parameters to find a good approximation. To make an example in spatially organized data, such as images and sequence of words, the number of weights that an MLP would require to find a good model would be intractable.

The term Deep Learning is often used to refer to the state of the art NN models that use different types of layers to tailor the model to the type of task that the network is supposed to learn.
DL models can use layers specifically designed to analyze sequences of data by using LSTMs and Attention layers. 
Other kinds of networks can use convolutional layers to extract features among spatially organized data such as 2D convolutions for images and 3D convolutions for videos.

In the field of Computer Vision, a convolution operation is a well-known operation used to extract interesting features in an image. 

The equation to compute a convolution on an image I and a kernel h is
\begin{equation}
 G(r,c) = (I \circledast h)(r,c) = \sum_{u=-L}^L \sum_{v=-L}^{L} I(r+u,c+v) \cdot h(-u, -v)
\end{equation}

For example when performing the edge detection the Canny edge detection algorithm use convolution operations to smooth the input image and detect the gradient wrt the x and y coordinates of the image. 


\begin{figure}[h!]
    \centering
    \begin{subfigure}[]{0.35\linewidth}
        \label{fig:CameramanSub}
        \includegraphics[width=\linewidth]{images/Cameraman.png}
    \end{subfigure}
    \hspace{1cm}
    \begin{subfigure}[]{.35\linewidth}
        \label{fig:CameramanDx}
        \includegraphics[width=\linewidth]{images/CameramanDx.png}
    \end{subfigure}
    \caption{Horizontal derivative with Sobel operator}
    \label{fig:Cameraman}
\end{figure}


In the DL field, a Convolutional Neural Network is a NN that uses convolutional layers, usually to analyze images.
In a CNN the network is organized as a sequence of convolutional layers usually followed by few fully connected layers.
The sequence of convolutional layers learn the filters to be extracted directly from the input data; the layers extract features of increasing complexity as the layers progress from the input to the output layer.
Between convolutional layers is common practice to use pooling layers. This is done to decrease the data dimensionality and be able to use more kernels to extract high-level features.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[]{0.4\linewidth}
        \label{fig:conv}
        \includegraphics[width=\linewidth]{images/Convolution.png}
        \caption{2D Convolution}
    \end{subfigure}
    \hspace{1cm}
    \begin{subfigure}[]{0.4\linewidth}
        \label{fig:avgPool}
        \includegraphics[width=\linewidth]{images/AVGPool.png}
        \caption{AVG Pool}
    \end{subfigure}
    \caption{Examples of 2D convolution (a) and AVG Pool operation (b). ~\cite{dumoulin2016guide}}
    \label{fig:ConvImgs}
\end{figure}

One interesting difference between how convolutions are typically handled and how a CNN handle convolution is how they are performed on images with multiple channels.
The convolution operation is performed as usual but in the end, all convolved channels are added pixelwise.
The number of channels of the output tensor is equal to the number of kernels considered by the convolution operation.

The CNN topology is inspired by how the animal visual cortex work; multiple neurons are connected in a hierarchical way to recognize features of increasing complexity. Neurons on the low level of the hierarchy recognize small features such as simple lines and corners; neurons at higher levels recognize more abstract features such as more complex shapes and objects.

\begin{figure}[h!]
  \includegraphics[width=0.9\textwidth]{images/lenet.jpg}
  \centering
  \caption{LeNet-5 architecture. Hand written digit recognition in 28x28 gray scale images. ~\cite{lenet}}
  \label{fig:lenet}
\end{figure}


\subsection{DL accelerators}

When a DL model must be deployed in a real system the forward pass can not always be done by simply using a CPU to perform the computation. The system is likely to have latency, throughput, and power constraints that a simple CPU implementation is not able to deliver. CPUs have limited throughput performance and can exploit a limited amount of coarse and fine-grained parallelism.

Since the training phase uses batches of data to calculate the gradient for the next weight update GPGPUs are the most common choice.
The high degree of parallelism that can be exploited from different weights and the different samples of the batch allows a GPU implementation to exploit all the available parallelism.
In the case of multiple GPUs available there are methods allow to use all the available computational power; multiple GPUs can be used to train bigger models with more parameters and use bigger batches to better approximate the true gradient.

Even if GPUs work great for parallelism when training a model they aren't always a good option when deploying a model.
GPUs are power-hungry and since have a dedicated memory to perform computation they require time-consuming memory transfers every time a kernel is executed.
This might be a problem, especially in latency constrained applications where the system is required to process a stream of data where each sample depends on the decisions that the model has made at previous steps.

Multiple solutions are available to solve the problem of delivering high throughput and low latency while not exploiting the parallelism created by processing batches of data. The obvious and best possible solution to solve the problem would be to use specialized ASICs to perform the computation. This would deliver the best possible results in terms of final system performance but have its drawbacks. 
The long design time of specialized HW solutions is impractical in situations where fast prototyping is a desirable feature of the design process. 
Moreover, ASIC solutions are not adaptable to new models; an ASIC would be fixed not able to implement new and yet undiscovered layers.

A different and more adaptable solution while preserving the low latency and high throughput performances on streams of data is to use reconfigurable HW.
A possibility is the use of FPGAs as deployment targets.
FPGAs are configurable devices that incorporate logic and memory blocks; the configuration can be designed to implement the specific function that needs to be computed by the specific application.
The use of FPGAs as targets allows the deployment of low-level HW while being able to deploy different models by just reconfiguring the HW configuration \cite{DBLP:journals/corr/abs-1901-04988}.

Another advantage of FPGAs over other solutions is the power efficiency attainable; as long as the implementation manages properly the memory, the use of FPGAs allows to maximize performance per watt.
Moreover, the reconfigurability allows to implement operations whit a floating point precision with an arbitrary number of bits; the HW solution is not restricted to use a number of bits that is a power of 2. This leads to a more tailored implementation improving area utilization and buffer sizes.

The disadvantages of FPGA implementations are usually limited to a maximum buffer size and the long design time required to create a working and efficient implementation.
The long design time can be reduced by using High-Level Synthesis tools that take as input a high-level specification of the computation (as an example the tool can take as input a filter written in C language) and output the bitstream ready to be deployed.

The result provided by an HLS tool does not have the same performance as a manually designed solution but the results are comparable. 
This also allows fast prototyping while enabling non HW experts to use specialized HW to deploy their trained models in real applications.

\section{ONNX}
\label{ONNX}

On the software side of the Deep Learning field, multiple frameworks emerged to ease the process of creating and training new models.
Since the DL field is relatively new there is no established intermediate representation to optimize the abstract computational graph before producing the actual implementation; different frameworks use different representations designed to work only with one framework.

\begin{figure}[h!]
  \includegraphics[width=0.9\textwidth]{images/ONNXExample.png}
  \centering
  \caption{Example of ONNX model.}
  \label{fig:onnx-example}
\end{figure}

The Open Neural Network eXchange (ONNX \cite{onnx}) has been created by Microsoft and Facebook to allow the portability of computational graphs between different frameworks. 
The goal is to create an intermediate representation that can be used by different frameworks and be used to implement optimizations common to all frameworks.

The ONNX representation can also be used as a common input representation for compilation stacks.
Since ONNX supports the translation from computational graphs of main DL frameworks to an ONNX representation, the compilation stack can avoid the burden of implementing a different input procedure for each input DL framework and just rely on the ONNX standard.
ONNX is also likely to be maintained and if new and more recent frameworks are going to be developed the stack can rely on ONNX to provide a translation from the new computational graph to the already specified representation.

\section{Conclusions}
This chapter has provided the concepts necessary for the thesis. 
We described the main characteristics of Artificial Intelligence and what lead to the Machine Learning field.
The chapter also described Deep Learning models and how they are implemented in the context of image processing applications.
The next chapter is dedicated to the exploration of tools related to the contribution of the thesis.


\end{document}